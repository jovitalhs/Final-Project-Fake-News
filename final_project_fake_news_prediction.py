# -*- coding: utf-8 -*-
"""Final_Project_Fake_News_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112jBBcmF2lz6mndcO3abPrPp7Lq41i3g

# 1.&nbsp;Import relevant libraries
"""

!pip install pandas numpy matplotlib seaborn nltk scikit-learn joblib streamlit pyngrok

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
import joblib

nltk.download('stopwords')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/news.csv')

# Import the dataset
file_dir = '/content/news.csv'
dataset = pd.read_csv(file_dir)

# Read Dataset
display(dataset)

"""Dataset ini memiliki 4 kolom. Kolom pertama berisi nomor seri berita. Kolom kedua berisi judul berita. Kolom ketiga berisi konten berita. Kolom keempat berisi pelabelan 'Fake' dan 'Real' untuk setiap berita."""

df.head()

df.shape

df.info()

"""Kolom tanpa nama tidak diperlukan untuk prediksi kami. Dan kolom judul, teks, dan label memiliki tipe data objek"""

# Drop unnecessary index column
df = df.drop(columns=['Unnamed: 0'])

df.columns

# Add text and title length for analysis
df['text_length'] = df['text'].apply(len)
df['title_length'] = df['title'].apply(len)

labels = df.label #Label ini nantinya akan digunakan sebagai target (y) dalam proses pelatihan model machine learning
labels.head()

df['label'].value_counts(normalize=True) * 100

"""Dataset memiliki persentase berita FAKE 50.05% dan REAL49.9% (hampir sama)."""

# EDA Summary
eda_summary = {}

# 1. Label distribution
label_counts = df['label'].value_counts()
eda_summary['Label Distribution'] = label_counts.to_dict()

# 2. Check for missing values
missing_values = df.isnull().sum()
eda_summary['Missing Values'] = missing_values[missing_values > 0].to_dict()

# 3. Text length statistics
text_length_stats = df['text_length'].describe()
eda_summary['Text Length Stats'] = text_length_stats.to_dict()

# 4. Title length statistics
title_length_stats = df['title_length'].describe()
eda_summary['Title Length Stats'] = title_length_stats.to_dict()

df['text_length'].describe()

"""count: Jumlah total data yang dianalisis (6335 teks).
mean	Rata-rata panjang teks adalah sekitar 4707 karakter.
std: Standard deviation = 5090, artinya panjang teks sangat bervariasi.
min: Teks terpendek hanya memiliki 1 karakter.
25% (Q1): 25% teks memiliki panjang ≤ 1741 karakter.
50% (median): Setengah teks memiliki panjang ≤ 3642 karakter.
75% (Q3): 75% teks memiliki panjang ≤ 6192 karakter.
max: Teks terpanjang memiliki 115,372 karakter, kemungkinan outlier.
"""

df['title_length'].describe()

# Plot label distribution
plt.figure(figsize=(6,4))
sns.countplot(x='label', data=df)
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

"""Distribusi Label:

FAKE: 3164

REAL: 3171

Dataset relatif seimbang antara berita FAKE dan REAL.
"""

# Plot title length distribution
plt.figure(figsize=(8,4))
sns.histplot(df['title_length'], bins=50, kde=True)
plt.title('Distribution of Title Length')
plt.xlabel('Number of Characters in Title')
plt.tight_layout()
plt.show()

"""Bentuk distribusi: Distribusi mendekati simetris seperti bentuk lonceng, maka bisa dikatakan mendekati normal.

Terdapat skew ke kanan (ekor panjang di kanan), berarti ada beberapa judul yang sangat panjang → right-skewed.
"""

eda_summary

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('/content/news.csv')

"""# 2.&nbsp;Data Preprocessing

"""

import pandas as pd
import string
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Load data
df = pd.read_csv('news.csv')

# Hapus kolom yang tidak perlu
df.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)

# Gabungkan judul dan isi berita
df['content'] = df['title'] + ' ' + df['text']

# Pre-processing: lowercase, hapus angka, tanda baca, dan stopwords
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = text.split()
    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]
    return ' '.join(tokens)

# Terapkan pre-processing
df['clean_content'] = df['content'].apply(clean_text)

# Ubah label ke angka
df['label'] = df['label'].map({'FAKE': 0, 'REAL': 1})

# Simpan hasil pre-processing jika perlu
# df.to_csv('news_cleaned.csv', index=False)

# Lihat hasil
print(df[['clean_content', 'label']].head())

"""##TF-IDF Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Pisahkan fitur dan label
X = df['clean_content']
y = df['label']

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000)  # Bisa disesuaikan jumlah fitur
X_tfidf = tfidf.fit_transform(X)

# Split dataset untuk training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Cek bentuk hasil TF-IDF
print("TF-IDF shape:", X_tfidf.shape)

"""# 3.&nbsp;Modeling (5 Models)"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Daftar model
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(),
    "SVM (Linear SVC)": LinearSVC(),
    "K-Nearest Neighbors": KNeighborsClassifier()
}

# Evaluasi semua model
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    })

# Konversi ke DataFrame untuk ditampilkan
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by="F1 Score", ascending=False)

# Tampilkan hasil evaluasi
print(results_df)

"""SVM (Linear SVC) memberikan performa terbaik secara keseluruhan, terutama pada F1 Score, sehingga kita akan menyimpan model ini sebagai model final.

# 4.&nbsp;Model Evaluation & Selection

##Visualisasi Hasil Evaluasi
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set(style="whitegrid")

# Buat plot F1 Score
plt.figure(figsize=(10, 6))
sns.barplot(data=results_df, x="Model", y="F1 Score", palette="Blues_d")
plt.title("Perbandingan F1 Score antar Model")
plt.xticks(rotation=30)
plt.ylim(0.80, 0.95)
plt.tight_layout()
plt.show()

"""##Menyimpan Model Terbaik"""

import joblib

# Simpan model terbaik
best_model = LinearSVC()
best_model.fit(X_train, y_train)

# Simpan model dan vectorizer ke file
joblib.dump(best_model, 'best_fake_news_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

"""## Membuat Fungsi Prediksi Baru"""

import joblib

# Load model dan vectorizer
model = joblib.load('best_fake_news_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Fungsi prediksi
def predict_fake_news(text):
    # Preprocessing (disesuaikan dengan yang kamu lakukan sebelumnya)
    import string
    import re
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

    # Cleaning function (tanpa stemming)
    def clean_text(text):
        text = text.lower()
        text = re.sub(r'\d+', '', text)
        text = text.translate(str.maketrans('', '', string.punctuation))
        tokens = text.split()
        tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]
        return ' '.join(tokens)

    # Bersihkan teks
    cleaned = clean_text(text)

    # Transformasi TF-IDF
    tfidf_input = vectorizer.transform([cleaned])

    # Prediksi
    prediction = model.predict(tfidf_input)[0]

    # Interpretasi hasil
    return "REAL" if prediction == 1 else "FAKE"

"""## Contoh Implementasi"""

text = "Donald Trump just announced a new global policy in a surprise speech."
print(predict_fake_news(text))